{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5baebd",
   "metadata": {},
   "source": [
    "<div align=\"middle\">\n",
    "  <h1><b><i>تمرین سوم</i></b></h1>\n",
    " </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324a8b6",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "   #  بخش اول:  هدوپ \n",
    "\n",
    "    ۱- به سوالات زیر پاسخ دهید\n",
    "    - مفهوم replication جیست\n",
    "    - مفهوم block در HDFS چیست و اگر بلاک‌ها را بسیار کوچک درنظر بگیریم چه مشکلی پیش می‌آید\n",
    "    \n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc900303",
   "metadata": {},
   "source": [
    "    Replication:\n",
    "    It is a techinque used in distributed databases to store multiple copies of a data table at different sites.\n",
    "    \n",
    "    Block:\n",
    "    Each block stores a part of our data. If block size is low, then we need a lot of processings to get all of our data\n",
    "    and it has a lot of overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088973c",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "\n",
    "    ۲- در  این قسمت شما باید ابتدا دیتاست داده شده را از حالت فشرده در بیاورید و سپس فایل‌های درون آن را در کلاستر هدوپ در مسیر /homework3/dataset/ بارگزاری کنید \n",
    "    نکته: در این بخش دستورات زده شده خود را برای کار با hdfs  در ترمینال را در سلول زیر وارد نمایید\n",
    "    برای دسترسی به کامند 'hdfs dfs'  میتوانید وارد یکی از کانتینر‌های هدوپ شوید و دستور را اجرا کنید همچنین همه کانتینر‌ها دارای shared_dir\n",
    "    در روت خود هستند و این دایرکتوری در تمام کانتینر‌ها به اشتراک گذاشته شده است\n",
    "    برای چک کردن فایل‌ها در hdfs به \n",
    "    \n",
    "[HDFS webUI](http://localhost:9870/explorer.html#/)\n",
    "    \n",
    "    مراجعه کنید\n",
    " </div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ae426a5",
   "metadata": {},
   "source": [
    "# first we need to find the docker container id\n",
    "docker ps\n",
    "# then we need to get inside of our container\n",
    "docker exec -it <id> bash\n",
    "# create a directory inside hadoop container for hdfs\n",
    "hdfs dfs -mkdir -p /usr/clsadmin/cloudcomputing\n",
    "# uploading files\n",
    "hdfs dfs -put shared_dir/dataset/War.json /usr/clsadmin/cloudcomputing\n",
    "hdfs dfs -put shared_dir/dataset/Weapon.json /usr/clsadmin/cloudcomputing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad56b8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "   #  بخش دوم:  اسپارک \n",
    "\n",
    "    ۱- به سوالات زیر پاسخ دهید\n",
    "    - مزیت اسپارک نسبت به مدل قدیمی map/reduce چیست؟\n",
    "    -  تفاوت action و transform در اسپارک چیست؟\n",
    "    \n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d3bd9",
   "metadata": {},
   "source": [
    "    Spark vs Map/Reduce:\n",
    "    Spark is much faster, because it processes data in memory (RAM). At the same time, Hadoop map/reduce\n",
    "    has to persist data back to the disk after every map or reduce action, but Spark uses RDD.\n",
    "    \n",
    "    Action vs Tranfrom:\n",
    "    If a function returns a DataFrame, Dataset, or RDD, it is a transformation. If it returns\n",
    "    anything else or does not return a value at all (or returns unit in the case of Scala API), it is an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477304bd",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "\n",
    "    ۲- کد‌های خواسته شده در قسمت‌های پایینی را تکمیل کنید\n",
    "    (قسمت های ToDo )\n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5e393",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "##  اتصال به کلاستر اسپارک و هدوپ \n",
    "\n",
    "    در این قسمت از تمرین باید به عنوان درایور یک سسشن  به کلاستر اسپارک بسازیم.\n",
    " </div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0c9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "import pandas , numpy\n",
    "import matplotlib\n",
    "import pprint\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e7dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.defaultFS\n",
      "23/01/23 19:35:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.20.30.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>homework3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62951ab550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"homework3\").master(\"spark://spark-master:7077\").config(\"fs.defaultFS\",\"hdfs://namenode:9000/\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22c68b",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  خواندن داده \n",
    "\n",
    "    :در اسپارک ما ساختارهای مختلفی برای کار با داده و پخش شدن آن‌ها در شبکه داریم که به ۳ دسته تقسیم بندی میشوند \n",
    "+ RDD\n",
    "+ Dataset\n",
    "+ DataFrame\n",
    "    \n",
    "    \n",
    "     برای مطالعه بیشتر به لینک زیر مراجعه کنید:\n",
    "[rdd-vs-dataframe-vs-dataset](https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset)\n",
    "\n",
    "    ما در درس با ساختار RDD آشنا شدیم حال در این تمرین میخواهیم با ساختار Dataframe آشنا شویم و به کمک آن دیتا را از روی HDFS بخوانیم و روی آن فایل‌ها پردازش انجام دهیم\n",
    " \n",
    " </div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061cf4d",
   "metadata": {},
   "source": [
    "- [Spark Cluster Master UI](http://localhost:8080/)\n",
    "- [Application master UI (driver UI)](http://localhost:4040)\n",
    "- [Web Hdfs](http://localhost:9870/explorer.html#/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73f6af",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " .در این قسمت دیتاست را لود می‌کنیم \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dc7ca3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "War = spark.read.json(f\"/usr/clsadmin/cloudcomputing/War.json\")\n",
    "Weapon = spark.read.json(\"/usr/clsadmin/cloudcomputing/Weapon.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4496700",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "برای اینکه بتوانیم روی دیتای لود شده به وسیله تابع  spark.sql\n",
    "    کوئری‌های SQL بزنیم\n",
    "    باید دو دیتاست لود شده را به عنوان table\n",
    "    به spark \n",
    "    معرفی کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c1c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "War.registerTempTable(\"War\")\n",
    "Weapon.registerTempTable(\"Weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fae11d",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "یک مثال ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec1f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "|  DateOfWar|DurationOfWar|Location|MinorityStart|TargetMinority|      Weapon|\n",
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "|73298-04-22|       1005.0|BIEN HOA|          Elf|           Orc|Mirkwood Bow|\n",
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM War where DurationOfWar=1005.0 limit 1;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d34c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "|  DateOfWar|DurationOfWar|    Location|MinorityStart|TargetMinority|      Weapon|\n",
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "|73361-06-05|       1005.0|TAN SON NHUT|          Elf|           Orc|Belthronding|\n",
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "War.select(\"*\").filter(col(\"DurationOfWar\")==1005.0).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb43b1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " ده رکورد آخر را نمایش دهید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75edd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DateOfWar='73361-08-27', DurationOfWar=800.0, Location='TAN SON NHUT', MinorityStart='Elf', TargetMinority='Orc', Weapon=None),\n",
       " Row(DateOfWar='73360-11-28', DurationOfWar=1522.0, Location='UBON AB', MinorityStart='Elf', TargetMinority='Goblin', Weapon='Andúril'),\n",
       " Row(DateOfWar='73360-05-23', DurationOfWar=1740.0, Location='UBON AB', MinorityStart='Elf', TargetMinority='Goblin', Weapon=None),\n",
       " Row(DateOfWar='73360-03-21', DurationOfWar=0.0, Location='DANANG', MinorityStart='Elf', TargetMinority='Goblin', Weapon='Nenya'),\n",
       " Row(DateOfWar='73360-07-24', DurationOfWar=0.0, Location='TONKIN GULF', MinorityStart='Elf', TargetMinority='Goblin', Weapon='Vilya'),\n",
       " Row(DateOfWar='73361-10-19', DurationOfWar=725.0, Location='DANANG', MinorityStart='Elf', TargetMinority='Goblin', Weapon='Andúril'),\n",
       " Row(DateOfWar='73362-01-21', DurationOfWar=1506.0, Location='UBON AB', MinorityStart='Elf', TargetMinority='Goblin', Weapon='Herugrim'),\n",
       " Row(DateOfWar='73361-07-01', DurationOfWar=1000.0, Location='NHA TRANG', MinorityStart='Orc', TargetMinority='Orc', Weapon='Mirkwood Bow'),\n",
       " Row(DateOfWar='73360-02-28', DurationOfWar=800.0, Location='PHAN RANG', MinorityStart='Elf', TargetMinority='Orc', Weapon='Aranrúth'),\n",
       " Row(DateOfWar='73361-08-13', DurationOfWar=1910.0, Location='KORAT', MinorityStart='Elf', TargetMinority='Goblin', Weapon=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "War.select(\"*\").tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ef302",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "اسکیما یا ساختار دیتاست ها را نمایش دهید\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64bf3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateOfWar: string (nullable = true)\n",
      " |-- DurationOfWar: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinorityStart: string (nullable = true)\n",
      " |-- TargetMinority: string (nullable = true)\n",
      " |-- Weapon: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Weapon: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "War.printSchema()\n",
    "Weapon.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474a171",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe734089",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک SQL \n",
    "\n",
    "    مهمترین قابلیت اسپارک این است که می‌تواند با خواندن فایل‌ها به صورت توزیع شده روی آن‌ها پردازش انجام دهد و این پردازش را برنامه ‌نویس میتواند با استفاده از دستورات SQL اعمال کند\n",
    "    در این بخش از شما انتظار می‌رود که به وسیله spark SQL  به اسپارک کوئری  بزنید . \n",
    "    \n",
    "    \n",
    " </div>\n",
    "\n",
    "[pyspark.sql.functions.col](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.count](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.groupBy](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143afea1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " چه تعداد نبرد در کل این دوران‌ها انجام شده است؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d510ec2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all Wars is 4400775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "number = War.select(\"*\").count()\n",
    "print(f\"Number of all Wars is {number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2285fe",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467eb96",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "هر نژاد در چه تعدادی نبرد مشارکت داشته اند  به صورت مرتب شده نمایش دهبد؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca947f5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'War' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mWar\u001b[49m\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinorityStart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'War' is not defined"
     ]
    }
   ],
   "source": [
    "War.groupBy(\"MinorityStart\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ab07c",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f9e52",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک toPandas \n",
    "\n",
    "    یکی ار قابلیت‌های اسپارک این است که می‌توان dataframe های آن را به \n",
    "    dataframe های pandas تبدیل کند و از توابع آن از جمله\n",
    "    توابع plot  آن برای رسم نمودار استفاده کرد.\n",
    "    در این قسمت از شما انتظار می‌رود نمودار تعداد جنگ‌ها بر اساس هر گونه را رسم کنید.\n",
    "    \n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[pyspark.sql.DataFrame.toPandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html)\n",
    "\n",
    "[pyspark.pandas.DataFrame.plot](https://spark.apache.org/docs/3.2.1/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.plot.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ea086",
   "metadata": {},
   "outputs": [],
   "source": [
    "War.groupBy(\"MinorityStart\").count().orderBy(col(\"count\").desc()).toPandas().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dcf3a",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/plotlib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9ec0c",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "چه تعداد نبردهایی بر اساس زمان انجام شده بر اساس زمان مرتب کنید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "War.groupBy(\"DateOfWar\").count().orderBy(col(\"DateOfWar\").asc()).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611b0b6",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/date_groupBy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7951b4",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "نژاد \"اورک\" توسط چه نژادی مورد حمله قرار گرفته است ؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b33625",
   "metadata": {},
   "outputs": [],
   "source": [
    "War.select(\"MinorityStart\").filter(col(\"TargetMinority\")==\"Orc\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b202",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/orc_target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32412b3c",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک Shuffle \n",
    "\n",
    "    Spark SQL shuffle مکانیزمی است برای توزیع مجدد یا پارتیشن بندی مجدد داده ها به طوری که داده ها به طور متفاوت در پارتیشن ها گروه بندی می شوند، بر اساس اندازه داده شما ممکن است نیاز باشد تعداد پارتیشن های RDD/DataFrame را با استفاده از اسپارک کاهش یا افزایش دهید.\n",
    "    برای مثال وقتی روی دو dataframe مختلف که روی شبکه توزیع شده اند\n",
    "    دستور join را میزنیم یک عملیات \n",
    "    shuffling انجام میشود\n",
    "    در این قسمت از شما انتظار می‌رود کوئری جوین زیر را نوشته و اجرا کنید همچنین به  صفحه \n",
    "    application master ui\n",
    "    مراجعه کنید و نحوه shuffleing را گزارش کنید . \n",
    "     و همچنین توضیح دهید DAG scheduler  در اسپارک چیست ؟\n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[shuffling in standalone cluster](https://medium.com/@rachit1arora/apache-spark-shuffle-service-there-are-more-than-one-options-c1a8e098230e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8069cdd",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "    بیشترین سلاحی که در این جنگ ها استفاده شده کدام سلاح  بوده است؟.\n",
    "    جزییات این سلاح را از فایل مشخصات سلاح (Weapon) می‌توانید بدست آورید.\n",
    "    فایل مربوط به سلاح ها را بازخوانی کرده و برای نمایش بین دو فایل از join استفاده شود.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c390e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DAG) Scheduler makes it possible to create a pipeline of models \n",
    "# for execution in a single client request. \n",
    "# The pipeline is a Directed Acyclic Graph with different nodes which define how to process each step of predict request.\n",
    "\n",
    "# lets make a join\n",
    "War.join(Weapon, War.Weapon == Weapon.Weapon).select(\"Weapon\", count(\"*\").alias(\"MissionsCount\"), \"Description\").groupBy(\"Weapon\").c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03487d2",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/most_used_weapons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894957ac",
   "metadata": {},
   "source": [
    "     # ToDo\n",
    "     They typically send messages of 2 types: RegisterExecutor and OpenBlocks. RegisterExecutor is used when the executor wants to register    within its local shuffle server. OpenBlocks message is used during blocks fetch process. Both actions are operated by BlockManager via its shuffleClient. Regarding the external shuffle service enabled configuration, the instance used in this field is either NettyBlockTransferService (no external shuffle) or org.apache.spark.network.shuffle.ExternalShuffleClient.\n",
    "     # Refere to Application master UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbad383",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "# تابع تعریف شده توسط کاربر(UDF)\n",
    "\n",
    "    یکی از مزایای اسپارک این است که نه تنها برای ما یک زبان SQL فراهم کرده که روی چندین سرور به صورت همزمان پردازش را انجام دهد بلکه \n",
    "    میتوان به زبان‌های مختلف توابعی تعریف کرد که روی  همه executor ها اجرا شود\n",
    "    در این بخش از شما انتظار می‌رود که با نوشتن یک UDF به زبان پایتونی \n",
    "    توضیحات هر اسلحه را به 3 کلمه اول آن کوتاه کنید و بقیه کلمات را حذف کنید و سوتونی به نام short_description را به داده های \n",
    "    Weapon اضافه کنید\n",
    "    \n",
    "    \n",
    " </div>\n",
    " \n",
    "[pyspark.sql.functions.udf](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.withColumn](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f5b20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we need to create our function\n",
    "@udf\n",
    "def make_short(s):\n",
    "    parts = s.split(' ')\n",
    "    return ' '.join(parts[:3])\n",
    "\n",
    "# using our udf\n",
    "Weapon.select(\"*\").withColumn('short_description', make_short(Weapon.Description)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697274a2",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/shortener_udf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedc77d",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "# نوشتن داده و پارتیشنینگ\n",
    "    زمانی که میخواهیم داده های حجیم را به صورت فایل هایی ذخیره کنیم  نمیتوانیم همه داده را در یک فایل بزرگ ذخیره کنیم به چند دلیل :\n",
    "-  ذخیره سازی یک فایل بزرگ باعث ؛تنها نقطه شکست میشود؛ و با حذف آن کل داده از دست میرود\n",
    "- جستجو در این یک فایل بزرگ که مرتب شده نیست دشوار و عملی نیست\n",
    "- آپدیت کردن سخت تر میشود\n",
    "پس تا حدودی حل این میشکل از راه حل های زیر استفاده میکنیم\n",
    "# پارتیشنینگ: \n",
    "-     بر اساس یک فیلد داده هارا دسته بندی میکنیم و در دایرکتوری های مختلف میریزیم این کار را اسپارک برای ما انجام میدهد\n",
    "\n",
    "    \n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83271fc7",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "\n",
    "    در این بخش شما باید یک دیتاست کامل بسازید بدین شکل که دیتاست War و Weapon\n",
    "     را با یکدیگر جوین کنید سپس همه آن رکورد‌هایی که  در توضیحات اسلحه آنها کلمه sword \n",
    "    نیامده است را فیلتر کرده و برا اساس گونه شروع کننده جنگ (MinorityStart) پارتیشن کنید\n",
    "    و روی HDFS\n",
    "      در مسیر /homework3/<student_number>/output/war_without_sword بنویسید\n",
    "    و ۲۰ رکورد اول آن را نمایش دهید\n",
    "    توجه کنید فایل‌های خروجی باید از نوع JSON باشند\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "[pyspark.sql.DataFrameWriter.partitionBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc34fab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76e7220e",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![war without sword](expected_answers/war_without_sword.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2907a46",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    \n",
    "  ### گزارش HDFS\n",
    "    با مراجعه به HDFS UI\n",
    "    خروجی پارتیشن شده مسیر بالا را مشاهده میکنید\n",
    "     -تعداد دایرکتوری‌های ایجاد شده چندتاست و دلیل آن چیست؟ \n",
    "    -در هر دایرکتوری ایجاد شده چند فایل JSON میبینید و دلیل تعدد این فایل‌ها چیست؟\n",
    "    - چگونه میتوان از اسپارک خواست تا از تعدد این فایل‌ها جلوگیری کند و یک فایل در این مسیر‌ها بریزد؟\n",
    "    - coalesce و repartition  در اسپارک چیستند و کاربر اصلی آنها چیست؟\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377e1cb",
   "metadata": {},
   "source": [
    "    # ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeca558",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "\n",
    "   ## coalesce\n",
    "    همان تسک بالا را انجام دهید  ولی  سعی کنید در هر دایرکتوری پارتیشن شده تنها یک فایل JSON ریخته شود.\n",
    "    نکته:  در مسیر جدید زیر آن را بنویسید.\n",
    "    /homework3/<student_number>/output/war_without_sword_v2/\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "[pyspark.sql.DataFrame.coalesce](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60017a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605870db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
